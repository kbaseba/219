{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Clustering using both image and text\n",
    "\n",
    "In part 1 and part 2, we have practived the art of clustering text and images separately.\n",
    "However, can we map image and text to the same space? In the Pokemon world, Pokedex\n",
    "catalogs Pokemon’s appearances and various metadata. We will build our Pokedex from image\n",
    "dataset link and meta metadata link. Fortunately, ECE 219 Gym kindly provides new Pokemon\n",
    "trainers with the helper code for data preprocessing and inferencing. Please find the code on\n",
    "Bruinlearn modules Week 4.\n",
    "\n",
    "We will use the pre-trained CLIP [8] to illustrate the idea of multimodal clustering. CLIP\n",
    "(Contrastive Language–Image Pretraining) is an innovative model developed by OpenAI, de-\n",
    "signed to understand and connect concepts from both text and images. CLIP is trained on a\n",
    "vast array of internet-sourced text-image pairs. This extensive training enables the model to\n",
    "understand a broad spectrum of visual concepts and their textual descriptions.\n",
    "\n",
    "CLIP consists of two primary components: a text encoder and an image encoder. The text\n",
    "encoder processes textual data, converting sentences and phrases into numerical representa-\n",
    "tions. Simultaneously, the image encoder transforms visual inputs into a corresponding set\n",
    "of numerical values. These encoders are trained to map both text and images into a shared\n",
    "embedding space, allowing the model to compare and relate the two different types of data di-\n",
    "rectly. The training employs a contrastive learning approach, where the model learns to match\n",
    "corresponding text and image pairs against numerous non-matching pairs. This approach helps\n",
    "the model in accurately associating images with their relevant textual descriptions and vice\n",
    "versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 26:\n",
    "Try to construct various text queries regarding types of Pokemon (such as ”type:\n",
    "Bug”, ”electric type Pok ́emon” or ”Pok ́emon with fire abilities”) to find the relevant images from\n",
    "the dataset. Once you have found the most suitable template for queries, please find the top five\n",
    "most relevant Pokemon for type Bug, Fire and Grass. For each of the constructed query, please plot\n",
    "the five most relevant Pokemon horizontally in one figure with following specifications:\n",
    "- the title of the figure should be the query you used;\n",
    "- the title of each Pokemon should be the name of the Pokemon and its first and second type.\n",
    "\n",
    "Repeat this process for Pokemon of Dark and Dragon types. Assess the effectiveness of your queries\n",
    "in these cases as well and try to explain any differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Q26\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 27:\n",
    "Randomly select 10 Pokemon images from the dataset and use CLIP to find the\n",
    "most relevant types (use your preferred template, e.g ”type: Bug”). For each selected Pokemon,\n",
    "please plot it and indicate:\n",
    "- its name and first and second type;\n",
    "- the five most relevant types predicted by CLIP and their predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Q27\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 28:\n",
    "In the first and second question, we investigated how CLIP creates ’clusters’ by\n",
    "mapping images and texts of various Pokemon into a high-dimensional space and explored neighbor-\n",
    "hood of these items in this space. For this question, please use t-SNE to visualize image clusters,\n",
    "specifically for Pokemon types Bug, Fire, and Grass. You can use scatter plot from python package\n",
    "plotly. For the visualization, color-code each point based on its first type type 1 using the ’color’\n",
    "argument, and label each point with the Pokemon’s name and types using ’hover name’. This\n",
    "will enable you to identify each Pokemon represented in your visualization. After completing the\n",
    "visualization, analyze it and discuss whether the clustering of Pokemon types make sense to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Q28\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the visualization, analyze it and discuss whether the clustering of Pokemon types make sense to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece219",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
